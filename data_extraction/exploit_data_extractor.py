import os
import pandas as pd
import re


def parse_log_line(line):
    # Regular expression to match the log format
    log_pattern = re.compile(
        r'\[(.*?)\]\s+"(.*?)"\s+(\d+\.\d+\.\d+\.\d+)\s+(\d+\.\d+\.\d+\.\d+)\s+(\d+)\s+(\d+)\s+(\S+)\s+"(.*?)"\s+"(.*?)"\s+"(.*?)"\s+"(.*?)"\s+(\d+)\s+(\d+)\s+"(.*?)"\s+"(.*?)"\s+"(.*?)"\s+"(.*?)"\s+"(.*?)"\s+"(.*?)"')

    match = log_pattern.match(line)
    if match:
        return {
            'dateTime': match.group(1),
            'userName': match.group(2),
            'sourceIP': match.group(3),
            'destinationIP': match.group(4),
            'port': match.group(5),
            'statusCode': match.group(6),
            'cacheResult': match.group(7),
            'httpRequest': match.group(8),
            'domainClassification': match.group(9),
            'riskClassification': match.group(10),
            'mimeType': match.group(11),
            'bytesSent': match.group(12),
            'bytesReceived': match.group(13),
            'userAgentString': match.group(14),
            'webReferrerString': match.group(15),
            'urlMeta1': match.group(16),
            'urlMeta2': match.group(17),
            'urlMeta3': match.group(18),
            'urlMeta4': match.group(19),
        }
    return None


def process_log_file(file_path):
    data = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            parsed_data = parse_log_line(line.strip())
            if parsed_data:
                data.append(parsed_data)
            else:
                print(f"Skipping malformed line in {file_path}: {line.strip()}")
    return pd.DataFrame(data)


# Define the folder containing log files
log_folder = os.path.join('../data', 'raw_data_exploit')
print(f"Log folder path: {log_folder}")
all_files = [os.path.join(log_folder, f) for f in os.listdir(log_folder) if f.endswith('.webgateway')]
print(f"Found {len(all_files)} log files.")

dataframes = [process_log_file(file) for file in all_files]

# Combine all DataFrames
final_df = pd.concat(dataframes, ignore_index=True)
print("Combined all DataFrames")

# Save the DataFrame to CSV
output_folder = "../data/cleaned_exploit"
os.makedirs(output_folder, exist_ok=True)
print(f"Output folder path checked/created: {output_folder}")
output_file = os.path.join(output_folder, "combined_log_data.csv")
print(output_file)
final_df.to_csv(output_file, index=False)

print(f"Processed {len(all_files)} files and saved combined data to {output_file}")
